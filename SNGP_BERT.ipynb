{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CfbfGEjDsOa1"
      },
      "outputs": [],
      "source": [
        "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "#!pip install transformers\n",
        "#! git clone https://github.com/jmunozmendi/SNGP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytlLUatGsOa1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKNaweJh487d"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD, Adam\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch\n",
        "import os\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "#import gpytorch\n",
        "import math\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgalF12jsOa2"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8sipErUsOa2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('dataset.tsv', sep='\\t')\n",
        "\n",
        "dataset['text'] = dataset['text'].astype(str)\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYyJDqADsOa3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBAatZhosOa3"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples,\n",
        "                           padding='max_length',  # Pad to max_length\n",
        "                           truncation=True,       # Truncate to max_length\n",
        "                           max_length=200)        # Specify the max length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbn3fXdtsOa3"
      },
      "outputs": [],
      "source": [
        "dataset['tokenized_text'] = dataset['text'].apply(lambda x: preprocess_function(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxBMAmnvsOa3"
      },
      "outputs": [],
      "source": [
        "input_ids = dataset['tokenized_text'].apply(lambda x: x['input_ids'])\n",
        "attention_masks = dataset['tokenized_text'].apply(lambda x: x['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DniudfYtsOa3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "input_ids_tensor = torch.tensor(input_ids)\n",
        "attention_masks_tensor = torch.tensor([item for item in attention_masks])\n",
        "labels_tensor = torch.tensor(dataset['label'].values)\n",
        "\n",
        "# Custom Dataset class\n",
        "class DistilBERTDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_masks, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a tuple where the first element is a tuple of input_ids and attention_mask,\n",
        "        # and the second element is labels\n",
        "        return (self.input_ids[idx], self.attention_masks[idx]), self.labels[idx]\n",
        "\n",
        "\n",
        "dataset = DistilBERTDataset(input_ids_tensor, attention_masks_tensor, labels_tensor)\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = train_test_split(dataset, train_size=train_size, test_size=test_size, stratify=labels_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNqvPQEL487e"
      },
      "source": [
        "## Creating the DistilBERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foEhZJLF487e"
      },
      "outputs": [],
      "source": [
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self, num_outputs = 768):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, num_outputs)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        (input_ids, attention_mask) = X\n",
        "\n",
        "        output_1 = self.l1(input_ids=input_ids.long(), attention_mask=attention_mask.long())\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        output = self.pre_classifier(pooler)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "num_inputs_gp = 768\n",
        "feature_extractor = DistillBERTClass(num_outputs = num_inputs_gp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ0bVBr4487e"
      },
      "source": [
        "## Creating the GP Layer\n",
        "\n",
        "In the next cell, we create the layer of Gaussian process models that are called after the neural network. In this case, we'll be using one GP per feature, as in the SV-DKL paper. The outputs of these Gaussian processes will the be mixed in the softmax likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ItwYtQB487e"
      },
      "outputs": [],
      "source": [
        "from SNGP.sngp import SNGP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUvYahnC487f"
      },
      "source": [
        "## Creating the full SVDKL Model\n",
        "\n",
        "With both the DenseNet feature extractor and GP layer defined, we can put them together in a single module that simply calls one and then the other, much like building any Sequential neural network in PyTorch. This completes defining our DKL model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDUgioB9487f"
      },
      "outputs": [],
      "source": [
        "model = SNGP(out_features=2, backbone=feature_extractor, backbone_output_features = num_inputs_gp, num_inducing=1024, momentum = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9jvDaPd_cAQ"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WVsv18a487f"
      },
      "source": [
        "## Defining Training and Testing Code\n",
        "\n",
        "Next, we define the basic optimization loop and testing code. This code is entirely analogous to the standard PyTorch training loop. We create a `torch.optim.SGD` optimizer with the parameters of the neural network on which we apply the standard amount of weight decay suggested from the paper, the parameters of the Gaussian process (from which we omit weight decay, as L2 regualrization on top of variational inference is not necessary), and the mixing parameters of the Softmax likelihood.\n",
        "\n",
        "We use the standard learning rate schedule from the paper, where we decrease the learning rate by a factor of ten 50% of the way through training, and again at 75% of the way through training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kWuurYQGjAg"
      },
      "outputs": [],
      "source": [
        "model.train_model(dataset=train_dataset, epochs=25, batch_size=32, weight_decay=0.01, lr = 2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7YoYUqUGjAh"
      },
      "outputs": [],
      "source": [
        "info = model.predict(dataset=test_dataset, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57tv01d4GjAh"
      },
      "outputs": [],
      "source": [
        "test_x, test_y = zip(*test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WdDgWTKGjAh"
      },
      "outputs": [],
      "source": [
        "test_x, test_y = np.array(test_x), np.array(test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqKoksZxGjAh"
      },
      "outputs": [],
      "source": [
        "result = np.logical_and(info.decision, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHgY7PqnGjAh"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy: \", result.sum() / len(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDmBwLaBGjAh"
      },
      "outputs": [],
      "source": [
        "info"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}